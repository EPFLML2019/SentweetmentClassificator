{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os.path\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#from src.blstm_tf import BiLSTM\n",
    "from src.implementations import batch_iter\n",
    "from src.blstm_pt import BiLSTM\n",
    "from scripts.tools import *\n",
    "\n",
    "# Data input and output paths\n",
    "POS_TRAIN_PATH = 'data/twitter-datasets/train_pos_full.txt' \n",
    "NEG_TRAIN_PATH = 'data/twitter-datasets/train_neg_full.txt' \n",
    "DATA_TEST_PATH = 'data/twitter-datasets/test_data.txt'\n",
    "OUTPUT_PATH = 'predictions_out.csv'\n",
    "TOKENS_PATH = \"saved_gen_files/all_tokens.npy\"\n",
    "W2V_MODEL_PATH = \"saved_gen_files/w2v.model\"\n",
    "FastText_MODEL_PATH = \"saved_gen_files/fasttext.model\"\n",
    "FULL_TRAIN_TWEET_VECTORS = \"saved_gen_files/all_tweets_vectors.npy\"\n",
    "FULL_TRAIN_TWEET_VECTORS_200 = \"saved_gen_files/all_tweets_vectors_200.npy\"\n",
    "TRAINING_DATA_PATH_X = 'data/training_data.npy'\n",
    "TRAINING_DATA_PATH_Y = 'data/data_y.npy'\n",
    "TRAINING_EMBEDDINGS = 'data/test_embeddings.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(TRAINING_DATA_PATH_X):\n",
    "    train_data = np.load(TRAINING_DATA_PATH_X)\n",
    "    train_y = np.load(TRAINING_DATA_PATH_Y)\n",
    "else:\n",
    "    embeddings = np.load('saved_gen_files/embeddings.npy')\n",
    "\n",
    "    train_text_neg = open(NEG_TRAIN_PATH, 'r').readlines()\n",
    "    train_text_pos = open(POS_TRAIN_PATH, 'r').readlines()\n",
    "    # Construct the two arrays \n",
    "    train_text = np.array(train_text_neg + train_text_pos)\n",
    "    train_y = np.concatenate([np.array([-1 for _ in range(len(train_text_neg))]), np.ones(len(train_text_pos))])\n",
    "\n",
    "    with open('saved_gen_files/vocab.pkl', 'rb') as f:\n",
    "        voc = pickle.load(f)\n",
    "\n",
    "    def toAvgVec(t):\n",
    "\n",
    "        _, K = embeddings.shape\n",
    "        sum_vec = np.zeros((K))\n",
    "        words = t.split()\n",
    "        for word in words:\n",
    "            index = voc.get(word)\n",
    "            if index is not None:\n",
    "                sum_vec += embeddings[index]\n",
    "\n",
    "        return sum_vec/len(words)\n",
    "    # Create numerical feature matrix of tweets\n",
    "    train_data = np.zeros(len(train_text)*embeddings.shape[1]).reshape(len(train_text), 20)\n",
    "    for i in range(len(train_text)):\n",
    "        train_data[i] = toAvgVec(train_text[i])\n",
    "    \n",
    "    np.save(TRAINING_DATA_PATH_X, train_data)\n",
    "    np.save(TRAINING_DATA_PATH_Y, train_y)\n",
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "random.shuffle(indices)\n",
    "\n",
    "indices\n",
    "X_train = train_data[indices[:2400000]]\n",
    "y_train = train_y[indices[:2400000]]\n",
    "\n",
    "X_test = train_data[2400000:]\n",
    "y_test = train_y[2400000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids, pos_text_train = load_csv_test_data(POS_TRAIN_PATH)\n",
    "neg_ids, neg_text_train = load_csv_test_data(NEG_TRAIN_PATH)\n",
    "full_dataset = np.concatenate((pos_text_train, neg_text_train), axis=None)\n",
    "full_labels = np.concatenate((np.ones(len(pos_text_train)), -np.ones(len(pos_text_train))), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_vectors = np.load(FULL_TRAIN_TWEET_VECTORS)\n",
    "#all_tokens = np.load(TOKENS_PATH)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tweets_vectors, full_labels, test_size=.2)\n",
    "X_full = all_tweets_vectors\n",
    "y_full = full_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n",
    "y_full[y_full == -1] = 0\n",
    "print(np.sum(y_train < 0), np.sum(y_test < 0), np.sum(y_full < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 50)\n",
      "(50000, 50)\n"
     ]
    }
   ],
   "source": [
    "ratio = .1\n",
    "train_size = int(X_train.shape[0] * ratio)\n",
    "test_size = int(X_test.shape[0] * ratio)\n",
    "X_train_reduced = X_train[:train_size]\n",
    "y_train_reduced = y_train[:train_size]\n",
    "X_test_reduced = X_test[:test_size]\n",
    "y_test_reduced = y_test[:test_size]\n",
    "\n",
    "print(X_train_reduced.shape)\n",
    "print(X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 1, 50)\n",
      "(50000, 1, 50)\n",
      "(2500000, 1, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train_reshape = np.reshape(X_train_reduced, (X_train_reduced.shape[0], 1, X_train_reduced.shape[1]))\n",
    "X_test_reshape = np.reshape(X_test_reduced, (X_test_reduced.shape[0], 1, X_test_reduced.shape[1]))\n",
    "X_full_reshape = np.reshape(X_full, (X_full.shape[0], 1, X_full.shape[1]))\n",
    "\n",
    "print(X_train_reshape.shape)\n",
    "print(X_test_reshape.shape)\n",
    "print(X_full_reshape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Bidirectional(LSTM(254)))\n",
    "model1.add(Dense(128))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(64))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Bidirectional(LSTM(128)))\n",
    "model1.add(Dense(64))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 50000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 20s 99us/sample - loss: 0.4843 - accuracy: 0.7524 - val_loss: 0.4461 - val_accuracy: 0.7767\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 0.4498 - accuracy: 0.7759 - val_loss: 0.4346 - val_accuracy: 0.7854\n",
      "CPU times: user 1min 20s, sys: 2.89 s, total: 1min 23s\n",
      "Wall time: 35.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6893cb3590>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model1.fit(X_train_reshape, y_train_reduced, batch_size=512, epochs=2, validation_data=(X_test_reshape, y_test_reduced), verbose=1, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250000 samples, validate on 250000 samples\n",
      "2250000/2250000 [==============================] - 164s 73us/sample - loss: 0.4301 - accuracy: 0.7922 - val_loss: 0.4980 - val_accuracy: 0.7196\n",
      "CPU times: user 6min 47s, sys: 16.8 s, total: 7min 4s\n",
      "Wall time: 2min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4f7c6a8450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model1.fit(X_full_reshape, y_full, batch_size=512, epochs=1, validation_split=.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection multiple                  183296    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  65        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "=================================================================\n",
      "Total params: 199,809\n",
      "Trainable params: 199,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize using Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ad32de7d84b97029\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ad32de7d84b97029\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_y = model1.predict([X_test_reshape], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_prob = None\n",
    "f1_max = 0\n",
    "\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    f1 = sklearn.metrics.f1_score(y_test_reduced, (pred_test_y.flatten() > thresh).astype(int))\n",
    "    print('F1 score at threshold {} is {}'.format(thresh, f1))\n",
    "    \n",
    "    if f1 > f1_max:\n",
    "        f1_max = f1\n",
    "        opt_prob = thresh\n",
    "        \n",
    "print('Optimal probabilty threshold is {} for maximum F1 score {}'.format(opt_prob, f1_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to predict\n",
    "test_ids, test_x = load_csv_test_data(DATA_TEST_PATH, has_ID=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets_vectors = np.load(\"test_embeddings.npy\")\n",
    "tshape = test_tweets_vectors.shape\n",
    "test_tweets_vectors = np.reshape(test_tweets_vectors, (tshape[0], 1, tshape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred_submission_y = model1.predict([test_tweets_vectors], batch_size=1024, verbose=1)\n",
    "pred_submission_y = pred_submission_y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "create_csv_submission(test_ids, predict_labels(pred_submission_y, opt_prob), OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 2\n",
    "model = BiLSTM(hidden_dim)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on veut pas utiliser direct tout le dataset\n",
    "ratio = 0.01\n",
    "train_size = int(X_train.shape[0] * ratio)\n",
    "test_size = int(X_test.shape[0] * ratio)\n",
    "X_train = X_train[:train_size]\n",
    "y_train = y_train[:train_size]\n",
    "X_test = X_test[:test_size]\n",
    "y_test = y_test[:test_size]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(y_test == predict_labels)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
