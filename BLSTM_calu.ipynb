{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os.path\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#from src.blstm_tf import BiLSTM\n",
    "from src.implementations import batch_iter\n",
    "from src.blstm_pt import BiLSTM\n",
    "from scripts.tools import *\n",
    "\n",
    "# Data input and output paths\n",
    "POS_TRAIN_PATH = 'data/twitter-datasets/train_pos_full.txt' \n",
    "NEG_TRAIN_PATH = 'data/twitter-datasets/train_neg_full.txt' \n",
    "DATA_TEST_PATH = 'data/twitter-datasets/test_data.txt'\n",
    "OUTPUT_PATH = 'predictions_out.csv'\n",
    "TOKENS_PATH = \"saved_gen_files/all_tokens.npy\"\n",
    "W2V_MODEL_PATH = \"saved_gen_files/w2v.model\"\n",
    "FastText_MODEL_PATH = \"saved_gen_files/fasttext.model\"\n",
    "FULL_TRAIN_TWEET_VECTORS = \"saved_gen_files/all_tweets_vectors.npy\"\n",
    "TRAINING_DATA_PATH_X = 'data/training_data.npy'\n",
    "TRAINING_DATA_PATH_Y = 'data/data_y.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-33a89624a894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/random.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(self, x, random)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# pick an element in x[:i+1] with which to exchange x[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/random.py\u001b[0m in \u001b[0;36m_randbelow\u001b[0;34m(self, n, int, maxsize, type, Method, BuiltinMethod)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     def _randbelow(self, n, int=int, maxsize=1<<BPF, type=type,\n\u001b[0m\u001b[1;32m    225\u001b[0m                    Method=_MethodType, BuiltinMethod=_BuiltinMethodType):\n\u001b[1;32m    226\u001b[0m         \u001b[0;34m\"Return a random int in the range [0,n).  Raises ValueError if n==0.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.isfile(TRAINING_DATA_PATH_X):\n",
    "    train_data = np.load(TRAINING_DATA_PATH_X)\n",
    "    train_y = np.load(TRAINING_DATA_PATH_Y)\n",
    "else:\n",
    "    embeddings = np.load('saved_gen_files/embeddings.npy')\n",
    "\n",
    "    train_text_neg = open(NEG_TRAIN_PATH, 'r').readlines()\n",
    "    train_text_pos = open(POS_TRAIN_PATH, 'r').readlines()\n",
    "    # Construct the two arrays \n",
    "    train_text = np.array(train_text_neg + train_text_pos)\n",
    "    train_y = np.concatenate([np.array([-1 for _ in range(len(train_text_neg))]), np.ones(len(train_text_pos))])\n",
    "\n",
    "    with open('saved_gen_files/vocab.pkl', 'rb') as f:\n",
    "        voc = pickle.load(f)\n",
    "\n",
    "    def toAvgVec(t):\n",
    "\n",
    "        _, K = embeddings.shape\n",
    "        sum_vec = np.zeros((K))\n",
    "        words = t.split()\n",
    "        for word in words:\n",
    "            index = voc.get(word)\n",
    "            if index is not None:\n",
    "                sum_vec += embeddings[index]\n",
    "\n",
    "        return sum_vec/len(words)\n",
    "    # Create numerical feature matrix of tweets\n",
    "    train_data = np.zeros(len(train_text)*embeddings.shape[1]).reshape(len(train_text), 20)\n",
    "    for i in range(len(train_text)):\n",
    "        train_data[i] = toAvgVec(train_text[i])\n",
    "    \n",
    "    np.save(TRAINING_DATA_PATH_X, train_data)\n",
    "    np.save(TRAINING_DATA_PATH_Y, train_y)\n",
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "random.shuffle(indices)\n",
    "\n",
    "indices\n",
    "X_train = train_data[indices[:2400000]]\n",
    "y_train = train_y[indices[:2400000]]\n",
    "\n",
    "X_test = train_data[2400000:]\n",
    "y_test = train_y[2400000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of dateutil.rrule failed: Traceback (most recent call last):\n",
      "  File \"/home/lucas/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/lucas/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/lucas/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/lucas/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 317, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/lucas/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 280, in update_instances\n",
      "    ref.__class__ = new\n",
      "TypeError: __class__ assignment: 'weekday' object layout differs from 'weekday'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "pos_ids, pos_text_train = load_csv_test_data(POS_TRAIN_PATH)\n",
    "neg_ids, neg_text_train = load_csv_test_data(NEG_TRAIN_PATH)\n",
    "full_dataset = np.concatenate((pos_text_train, neg_text_train), axis=None)\n",
    "full_labels = np.concatenate((np.ones(len(pos_text_train)), -np.ones(len(pos_text_train))), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_vectors = np.load(FULL_TRAIN_TWEET_VECTORS)\n",
    "#all_tokens = np.load(TOKENS_PATH)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tweets_vectors, full_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n",
    "print(np.sum(y_train < 0), np.sum(y_test < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1875000, 50)\n",
      "(625000, 50)\n"
     ]
    }
   ],
   "source": [
    "ratio = 1\n",
    "train_size = int(X_train.shape[0] * ratio)\n",
    "test_size = int(X_test.shape[0] * ratio)\n",
    "X_train_reduced = X_train[:train_size]\n",
    "y_train_reduced = y_train[:train_size]\n",
    "X_test_reduced = X_test[:test_size]\n",
    "y_test_reduced = y_test[:test_size]\n",
    "\n",
    "print(X_train_reduced.shape)\n",
    "print(X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1875000, 1, 50)\n",
      "(625000, 1, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train_reshape = np.reshape(X_train_reduced, (X_train_reduced.shape[0], 1, X_train_reduced.shape[1]))\n",
    "X_test_reshape = np.reshape(X_test_reduced, (X_test_reduced.shape[0], 1, X_test_reduced.shape[1]))\n",
    "\n",
    "print(X_train_reshape.shape)\n",
    "print(X_test_reshape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model1.add(Dense(64))\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Bidirectional(LSTM(128)))\n",
    "model1.add(Dense(64))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1875000 samples, validate on 625000 samples\n",
      "Epoch 1/50\n",
      "1875000/1875000 [==============================] - 44s 24us/sample - loss: 0.4381 - accuracy: 0.7842 - val_loss: 0.4276 - val_accuracy: 0.7849\n",
      "Epoch 2/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.4148 - accuracy: 0.7989 - val_loss: 0.4062 - val_accuracy: 0.8028\n",
      "Epoch 3/50\n",
      "1875000/1875000 [==============================] - 43s 23us/sample - loss: 0.4071 - accuracy: 0.8037 - val_loss: 0.4003 - val_accuracy: 0.8057\n",
      "Epoch 4/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.4018 - accuracy: 0.8065 - val_loss: 0.3983 - val_accuracy: 0.8067\n",
      "Epoch 5/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3978 - accuracy: 0.8089 - val_loss: 0.3961 - val_accuracy: 0.8081\n",
      "Epoch 6/50\n",
      "1875000/1875000 [==============================] - 42s 23us/sample - loss: 0.3948 - accuracy: 0.8107 - val_loss: 0.3942 - val_accuracy: 0.8092\n",
      "Epoch 7/50\n",
      "1875000/1875000 [==============================] - 43s 23us/sample - loss: 0.3925 - accuracy: 0.8121 - val_loss: 0.3920 - val_accuracy: 0.8104\n",
      "Epoch 8/50\n",
      "1875000/1875000 [==============================] - 43s 23us/sample - loss: 0.3899 - accuracy: 0.8133 - val_loss: 0.3914 - val_accuracy: 0.8111\n",
      "Epoch 9/50\n",
      "1875000/1875000 [==============================] - 43s 23us/sample - loss: 0.3880 - accuracy: 0.8147 - val_loss: 0.3898 - val_accuracy: 0.8121\n",
      "Epoch 10/50\n",
      "1875000/1875000 [==============================] - 44s 23us/sample - loss: 0.3865 - accuracy: 0.8152 - val_loss: 0.3889 - val_accuracy: 0.8128\n",
      "Epoch 11/50\n",
      "1875000/1875000 [==============================] - 43s 23us/sample - loss: 0.3848 - accuracy: 0.8163 - val_loss: 0.3883 - val_accuracy: 0.8131\n",
      "Epoch 12/50\n",
      "1875000/1875000 [==============================] - 42s 23us/sample - loss: 0.3836 - accuracy: 0.8170 - val_loss: 0.3881 - val_accuracy: 0.8133\n",
      "Epoch 13/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3824 - accuracy: 0.8174 - val_loss: 0.3890 - val_accuracy: 0.8126\n",
      "Epoch 14/50\n",
      "1875000/1875000 [==============================] - 44s 23us/sample - loss: 0.3815 - accuracy: 0.8179 - val_loss: 0.3869 - val_accuracy: 0.8140\n",
      "Epoch 15/50\n",
      "1875000/1875000 [==============================] - 42s 23us/sample - loss: 0.3803 - accuracy: 0.8186 - val_loss: 0.3882 - val_accuracy: 0.8138\n",
      "Epoch 16/50\n",
      "1875000/1875000 [==============================] - 43s 23us/sample - loss: 0.3792 - accuracy: 0.8194 - val_loss: 0.3868 - val_accuracy: 0.8142\n",
      "Epoch 17/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3785 - accuracy: 0.8196 - val_loss: 0.3876 - val_accuracy: 0.8140\n",
      "Epoch 18/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3778 - accuracy: 0.8200 - val_loss: 0.3861 - val_accuracy: 0.8150\n",
      "Epoch 19/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3768 - accuracy: 0.8206 - val_loss: 0.3882 - val_accuracy: 0.8133\n",
      "Epoch 20/50\n",
      "1875000/1875000 [==============================] - 43s 23us/sample - loss: 0.3762 - accuracy: 0.8209 - val_loss: 0.3863 - val_accuracy: 0.8146\n",
      "Epoch 21/50\n",
      "1875000/1875000 [==============================] - 43s 23us/sample - loss: 0.3757 - accuracy: 0.8212 - val_loss: 0.3873 - val_accuracy: 0.8150\n",
      "Epoch 22/50\n",
      "1875000/1875000 [==============================] - 44s 23us/sample - loss: 0.3749 - accuracy: 0.8214 - val_loss: 0.3885 - val_accuracy: 0.8129\n",
      "Epoch 23/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3743 - accuracy: 0.8217 - val_loss: 0.3883 - val_accuracy: 0.8140\n",
      "Epoch 24/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3738 - accuracy: 0.8222 - val_loss: 0.3867 - val_accuracy: 0.8148\n",
      "Epoch 25/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3734 - accuracy: 0.8222 - val_loss: 0.3865 - val_accuracy: 0.8149\n",
      "Epoch 26/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3728 - accuracy: 0.8228 - val_loss: 0.3878 - val_accuracy: 0.8143\n",
      "Epoch 27/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3724 - accuracy: 0.8229 - val_loss: 0.3865 - val_accuracy: 0.8146\n",
      "Epoch 28/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3719 - accuracy: 0.8231 - val_loss: 0.3875 - val_accuracy: 0.8146\n",
      "Epoch 29/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3714 - accuracy: 0.8235 - val_loss: 0.3867 - val_accuracy: 0.8151\n",
      "Epoch 30/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3711 - accuracy: 0.8235 - val_loss: 0.3861 - val_accuracy: 0.8150\n",
      "Epoch 31/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3706 - accuracy: 0.8237 - val_loss: 0.3873 - val_accuracy: 0.8150\n",
      "Epoch 32/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3703 - accuracy: 0.8240 - val_loss: 0.3871 - val_accuracy: 0.8148\n",
      "Epoch 33/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3698 - accuracy: 0.8243 - val_loss: 0.3881 - val_accuracy: 0.8145\n",
      "Epoch 34/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3694 - accuracy: 0.8242 - val_loss: 0.3877 - val_accuracy: 0.8150\n",
      "Epoch 35/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3693 - accuracy: 0.8247 - val_loss: 0.3869 - val_accuracy: 0.8151\n",
      "Epoch 36/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3688 - accuracy: 0.8246 - val_loss: 0.3875 - val_accuracy: 0.8155\n",
      "Epoch 37/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3684 - accuracy: 0.8249 - val_loss: 0.3884 - val_accuracy: 0.8145\n",
      "Epoch 38/50\n",
      "1875000/1875000 [==============================] - 40s 22us/sample - loss: 0.3682 - accuracy: 0.8252 - val_loss: 0.3897 - val_accuracy: 0.8146\n",
      "Epoch 39/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3679 - accuracy: 0.8252 - val_loss: 0.3871 - val_accuracy: 0.8148\n",
      "Epoch 40/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3676 - accuracy: 0.8255 - val_loss: 0.3888 - val_accuracy: 0.8147\n",
      "Epoch 41/50\n",
      "1875000/1875000 [==============================] - 42s 22us/sample - loss: 0.3674 - accuracy: 0.8256 - val_loss: 0.3888 - val_accuracy: 0.8150\n",
      "Epoch 42/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3670 - accuracy: 0.8257 - val_loss: 0.3882 - val_accuracy: 0.8146\n",
      "Epoch 43/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3667 - accuracy: 0.8255 - val_loss: 0.3891 - val_accuracy: 0.8146\n",
      "Epoch 44/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3665 - accuracy: 0.8259 - val_loss: 0.3886 - val_accuracy: 0.8152\n",
      "Epoch 45/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3663 - accuracy: 0.8260 - val_loss: 0.3879 - val_accuracy: 0.8143\n",
      "Epoch 46/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3661 - accuracy: 0.8262 - val_loss: 0.3884 - val_accuracy: 0.8145\n",
      "Epoch 47/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3657 - accuracy: 0.8261 - val_loss: 0.3882 - val_accuracy: 0.8146\n",
      "Epoch 48/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3655 - accuracy: 0.8264 - val_loss: 0.3894 - val_accuracy: 0.8148\n",
      "Epoch 49/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3654 - accuracy: 0.8265 - val_loss: 0.3895 - val_accuracy: 0.8142\n",
      "Epoch 50/50\n",
      "1875000/1875000 [==============================] - 41s 22us/sample - loss: 0.3650 - accuracy: 0.8266 - val_loss: 0.3906 - val_accuracy: 0.8139\n",
      "CPU times: user 2h 51min 42s, sys: 6min 19s, total: 2h 58min 1s\n",
      "Wall time: 34min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd7c00dcf90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model1.fit(X_train_reshape, y_train_reduced, batch_size=512, epochs=50, validation_data=(X_test_reshape, y_test_reduced), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_y = model1.predict([X_test_reshape], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.787921188731107\n",
      "F1 score at threshold 0.11 is 0.790393270824673\n",
      "F1 score at threshold 0.12 is 0.7927165270577435\n",
      "F1 score at threshold 0.13 is 0.7950159105278979\n",
      "F1 score at threshold 0.14 is 0.7972194198012198\n",
      "F1 score at threshold 0.15 is 0.7992691326247007\n",
      "F1 score at threshold 0.16 is 0.8012368733334997\n",
      "F1 score at threshold 0.17 is 0.8031721467976158\n",
      "F1 score at threshold 0.18 is 0.8050091201538819\n",
      "F1 score at threshold 0.19 is 0.8067156331591384\n",
      "F1 score at threshold 0.2 is 0.8082804134314328\n",
      "F1 score at threshold 0.21 is 0.809769630140875\n",
      "F1 score at threshold 0.22 is 0.8112997893928621\n",
      "F1 score at threshold 0.23 is 0.8127259835974281\n",
      "F1 score at threshold 0.24 is 0.8141564657696156\n",
      "F1 score at threshold 0.25 is 0.8153650621290458\n",
      "F1 score at threshold 0.26 is 0.8165788088631208\n",
      "F1 score at threshold 0.27 is 0.8176142074354239\n",
      "F1 score at threshold 0.28 is 0.8186822895000815\n",
      "F1 score at threshold 0.29 is 0.8196372531888869\n",
      "F1 score at threshold 0.3 is 0.8205089303023406\n",
      "F1 score at threshold 0.31 is 0.8213093833114048\n",
      "F1 score at threshold 0.32 is 0.8219945721488531\n",
      "F1 score at threshold 0.33 is 0.8226705272026654\n",
      "F1 score at threshold 0.34 is 0.8229178904537444\n",
      "F1 score at threshold 0.35 is 0.8232871642977376\n",
      "F1 score at threshold 0.36 is 0.8235958113122355\n",
      "F1 score at threshold 0.37 is 0.8238336467655396\n",
      "F1 score at threshold 0.38 is 0.8240553797121366\n",
      "F1 score at threshold 0.39 is 0.8242889994195913\n",
      "F1 score at threshold 0.4 is 0.824134365398251\n",
      "F1 score at threshold 0.41 is 0.824040363245452\n",
      "F1 score at threshold 0.42 is 0.8238644419933702\n",
      "F1 score at threshold 0.43 is 0.8233566912254788\n",
      "F1 score at threshold 0.44 is 0.8228368385381838\n",
      "F1 score at threshold 0.45 is 0.8223771086495232\n",
      "F1 score at threshold 0.46 is 0.8217948580962166\n",
      "F1 score at threshold 0.47 is 0.8212691492954288\n",
      "F1 score at threshold 0.48 is 0.8205151954405189\n",
      "F1 score at threshold 0.49 is 0.8194441856275894\n",
      "F1 score at threshold 0.5 is 0.8183482569655767\n",
      "Optimal probabilty threshold is 0.39 for maximum F1 score 0.8242889994195913\n"
     ]
    }
   ],
   "source": [
    "opt_prob = None\n",
    "f1_max = 0\n",
    "\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    f1 = sklearn.metrics.f1_score(y_test_reduced, (pred_test_y.flatten() > thresh).astype(int))\n",
    "    print('F1 score at threshold {} is {}'.format(thresh, f1))\n",
    "    \n",
    "    if f1 > f1_max:\n",
    "        f1_max = f1\n",
    "        opt_prob = thresh\n",
    "        \n",
    "print('Optimal probabilty threshold is {} for maximum F1 score {}'.format(opt_prob, f1_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to predict\n",
    "test_ids, test_x = load_csv_test_data(DATA_TEST_PATH, has_ID=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_embeddings.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-07fc8763be63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_tweets_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_embeddings.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_tweets_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_tweets_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tweets_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_embeddings.npy'"
     ]
    }
   ],
   "source": [
    "test_tweets_vectors = np.load(\"test_embeddings.npy\")\n",
    "tshape = test_tweets_vectors.shape\n",
    "test_tweets_vectors = np.reshape(test_tweets_vectors, (tshape[0], 1, tshape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred_submission_y = model1.predict([test_tweets_vectors], batch_size=1024, verbose=1)\n",
    "pred_submission_y = pred_submission_y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "create_csv_submission(test_ids, predict_labels(pred_submission_y, opt_prob), OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3173489ec2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MLProject2/src/blstm_pt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_dim, num_features)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBiLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "hidden_dim = 2\n",
    "model = BiLSTM(hidden_dim)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 20)\n",
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Si on veut pas utiliser direct tout le dataset\n",
    "ratio = 0.01\n",
    "train_size = int(X_train.shape[0] * ratio)\n",
    "test_size = int(X_test.shape[0] * ratio)\n",
    "X_train = X_train[:train_size]\n",
    "y_train = y_train[:train_size]\n",
    "X_test = X_test[:test_size]\n",
    "y_test = y_test[:test_size]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(y_test == predict_labels)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
