{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2\n",
    "\n",
    "from embeddings import *\n",
    "from tools import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os.path\n",
    "from tokenizer import *\n",
    "\n",
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import *\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "\n",
    "# Data input and output paths\n",
    "POS_TRAIN_PATH = '../data/twitter-datasets/train_pos_full.txt' \n",
    "NEG_TRAIN_PATH = '../data/twitter-datasets/train_neg_full.txt' \n",
    "DATA_TEST_PATH = '../data/twitter-datasets/test_data.txt'\n",
    "OUTPUT_PATH = 'predictions_out.csv'\n",
    "\n",
    "TOKENS_PATH = \"../saved_gen_files/all_tokens.txt\"\n",
    "FULL_TRAIN_TWEET_VECTORS = \"../saved_gen_files/train_tweet_vectors.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids, pos_text_train = load_csv_test_data(POS_TRAIN_PATH)\n",
    "neg_ids, neg_text_train = load_csv_test_data(NEG_TRAIN_PATH)\n",
    "full_dataset = np.concatenate((pos_text_train, neg_text_train), axis=None)\n",
    "full_labels = np.concatenate((np.ones(len(pos_text_train)), -np.ones(len(pos_text_train))), axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(TOKENS_PATH):\n",
    "    with open(TOKENS_PATH, 'rb') as f:\n",
    "        all_tokens = pickle.load(f)\n",
    "else:    \n",
    "    all_tokens = [tokenize(tweet) for tweet in full_dataset]\n",
    "    \n",
    "    with open(TOKENS_PATH, 'wb') as f:\n",
    "        pickle.dump(all_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Use Bigram and Trigram only with self trained Glove on Bigram and Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigrams\n",
    "#all_tokens = computeBigrams(all_tokens)\n",
    "\n",
    "# Generate Trigrams\n",
    "#all_tokens = computeBigrams(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "Choose one of the embedding algo and the dimensions of the vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 50\n",
    "BIGRAM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "#### Self-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = getWord2VecDict(all_tokens, size=DIM, window=10, min_count=2, workers=10, iters=10, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove\n",
    "\n",
    "#### Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "wv = api.load(\"glove-twitter-\" + str(DIM)).wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "if BIGRAM:\n",
    "    glove_file = '../data/self_trained_gloves/vectors_bigram_d'+str(DIM)+'.txt'\n",
    "else: \n",
    "    glove_file = '../data/self_trained_gloves/vectors_d'+str(DIM)+'.txt'\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "\n",
    "# Normalize \n",
    "wv.init_sims(replace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.1\n",
    "train_size=0.1\n",
    "DIM = 100\n",
    "BIGRAM = True\n",
    "\n",
    "\n",
    "from embeddings import *\n",
    "from tools import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os.path\n",
    "from tokenizer import *\n",
    "\n",
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import *\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Data input and output paths\n",
    "POS_TRAIN_PATH = '../data/twitter-datasets/train_pos_full.txt' \n",
    "NEG_TRAIN_PATH = '../data/twitter-datasets/train_neg_full.txt' \n",
    "DATA_TEST_PATH = '../data/twitter-datasets/test_data.txt'\n",
    "OUTPUT_PATH = 'predictions_out.csv'\n",
    "\n",
    "TOKENS_PATH = \"../saved_gen_files/all_tokens.txt\"\n",
    "FULL_TRAIN_TWEET_VECTORS = \"../saved_gen_files/train_tweet_vectors.txt\"\n",
    "\n",
    "\n",
    "pos_ids, pos_text_train = load_csv_test_data(POS_TRAIN_PATH)\n",
    "neg_ids, neg_text_train = load_csv_test_data(NEG_TRAIN_PATH)\n",
    "full_dataset = np.concatenate((pos_text_train, neg_text_train), axis=None)\n",
    "full_labels = np.concatenate((np.ones(len(pos_text_train)), -np.ones(len(pos_text_train))), axis=None)\n",
    "\n",
    "\n",
    "with open(TOKENS_PATH, 'rb') as f:\n",
    "        all_tokens = pickle.load(f)\n",
    "\n",
    "\n",
    "# Generate bigrams\n",
    "#all_tokens = computeBigrams(all_tokens)\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "if BIGRAM:\n",
    "    glove_file = '../data/self_trained_gloves/vectors_bigram_d'+str(DIM)+'.txt'\n",
    "else:\t\n",
    "    glove_file = '../data/self_trained_gloves/vectors_d'+str(DIM)+'.txt'\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "\n",
    "# Normalize \n",
    "wv.init_sims(replace=True)\n",
    "\n",
    "labels = full_labels\n",
    "labels[labels<0] = 0\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(all_tokens, labels, test_size=test_size, train_size=train_size, random_state=1)\n",
    "\n",
    "\n",
    "#use_tensorboard = True\n",
    "\n",
    "#######################################\n",
    "####\t\t     sep-CNN\t\t   ####\n",
    "#######################################\n",
    "#from sepCNN import *\n",
    "#model= sepCNN_Model(all_tokens, tensorboard=True, useBigrams=BIGRAM)\n",
    "#model.train_model(X_train, y_train, wv, batch_size=128, epochs=5)\n",
    "#model.save(\"sepCnn_bigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 100)          12824500  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128, 128)          84480     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128, 32)           4128      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 12,917,205\n",
      "Trainable params: 12,917,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2250000 samples, validate on 250000 samples\n",
      "Epoch 1/5\n",
      "2250000/2250000 [==============================] - 7235s 3ms/sample - loss: 0.3305 - acc: 0.8517 - val_loss: 0.3798 - val_acc: 0.8111\n",
      "Epoch 2/5\n",
      "2250000/2250000 [==============================] - 7329s 3ms/sample - loss: 0.2867 - acc: 0.8751 - val_loss: 0.3500 - val_acc: 0.8330\n",
      "Epoch 3/5\n",
      "2250000/2250000 [==============================] - 7313s 3ms/sample - loss: 0.2694 - acc: 0.8842 - val_loss: 0.3616 - val_acc: 0.8303\n",
      "Epoch 4/5\n",
      "2250000/2250000 [==============================] - 7409s 3ms/sample - loss: 0.2579 - acc: 0.8898 - val_loss: 0.3380 - val_acc: 0.8492\n",
      "Epoch 5/5\n",
      "2250000/2250000 [==============================] - 10898s 5ms/sample - loss: 0.2492 - acc: 0.8941 - val_loss: 0.3680 - val_acc: 0.8379\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from lstm import LSTM_Model\n",
    "model= LSTM_Model(all_tokens, wv, tensorboard=False, useBigrams=BIGRAM)\n",
    "model.train_model(all_tokens, labels, wv, batch_size=128, epochs=5)\n",
    "model.save(\"blstm_bigram_100_5epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250000 samples, validate on 250000 samples\n",
      "Epoch 1/2\n",
      "2250000/2250000 [==============================] - 10750s 5ms/sample - loss: 0.2426 - acc: 0.8972 - val_loss: 0.3439 - val_acc: 0.8473\n",
      "Epoch 2/2\n",
      "2250000/2250000 [==============================] - 10948s 5ms/sample - loss: 0.2373 - acc: 0.9001 - val_loss: 0.3355 - val_acc: 0.8554\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "'''\n",
    "tweetsTokenized = [model.bigram[tweet] for tweet in all_tokens]\n",
    "tweetsTokenized = [list(filter(lambda i: i in wv, tweet)) for tweet in tweetsTokenized]\n",
    "\n",
    "# Transform each unique word in unique int identifier\n",
    "sequences = model.tokenizer_obj.texts_to_sequences(tweetsTokenized)\n",
    "'''\n",
    "# Pad the tweet to have all the same size\n",
    "#tweet_padded = pad_sequences(sequences, maxlen=model.max_length)\n",
    "\n",
    "\n",
    "model.model.fit(tweet_padded, labels, batch_size=128, epochs=2, validation_split=0.1, shuffle=True)\n",
    "\n",
    "model.save(\"blstm_bigram_100_7epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.85      0.83      0.84    124970\n",
      "         1.0       0.84      0.86      0.84    125030\n",
      "\n",
      "    accuracy                           0.84    250000\n",
      "   macro avg       0.84      0.84      0.84    250000\n",
      "weighted avg       0.84      0.84      0.84    250000\n",
      "\n",
      "0.843104\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "y_test[y_test ==0] = -1\n",
    "print(classification_report(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a word2vec model to generate embedding\n",
    "wv = getFasttextDict(all_tokens, size=DIM, window=10, min_count=2, workers=10, iters=10, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweet in features with previous embedding system\n",
    "all_tweets_vectors = generateTweetsFeatures(all_tokens, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 1., 0., 1.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tweets_vectors, full_labels, test_size=0.1, train_size=0.1, random_state=1)\n",
    "\n",
    "clf = load('svm_bigram.joblib') \n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.73      0.77    124970\n",
      "         1.0       0.76      0.85      0.80    125030\n",
      "\n",
      "    accuracy                           0.79    250000\n",
      "   macro avg       0.79      0.79      0.79    250000\n",
      "weighted avg       0.79      0.79      0.79    250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions[predictions<0] = 0\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.77      0.72      0.75    124637\n",
      "         1.0       0.74      0.79      0.77    125363\n",
      "\n",
      "    accuracy                           0.76    250000\n",
      "   macro avg       0.76      0.76      0.76    250000\n",
      "weighted avg       0.76      0.76      0.76    250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# Train and test the model\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "#clf = linear_model.Ridge(alpha=0.1)\n",
    "#clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(y_test, predict_labels(predictions)))\n",
    "#72-82\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2d4ef5df3e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpredict_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    144\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    145\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(gamma='scale')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "predict_svm = clf_svm.predict(X_test)\n",
    "print(classification_report(y_test, predict_labels(predict_svm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [list(filter(lambda i: i in wv, tweet)) for tweet in all_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the negative sentiment to be clasified to 0 instead of 1 \n",
    "labels = full_labels\n",
    "labels[labels<0] = 0\n",
    "use_tensorboard = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tensorboard:\n",
    "    %tensorboard --logdir logs\n",
    "    \n",
    "    ## Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tokens, labels, test_size=0.1, train_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 128, 200)          10947600  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128, 128)          135680    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128, 32)           4128      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,091,505\n",
      "Trainable params: 11,091,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2250000 samples, validate on 250000 samples\n",
      "Epoch 1/5\n",
      "2250000/2250000 [==============================] - 9831s 4ms/sample - loss: 0.3205 - acc: 0.8574 - val_loss: 0.3585 - val_acc: 0.8331\n",
      "Epoch 2/5\n",
      "2250000/2250000 [==============================] - 9854s 4ms/sample - loss: 0.2772 - acc: 0.8803 - val_loss: 0.3265 - val_acc: 0.8487\n",
      "Epoch 3/5\n",
      "2250000/2250000 [==============================] - 16608s 7ms/sample - loss: 0.2603 - acc: 0.8891 - val_loss: 0.3218 - val_acc: 0.8543\n",
      "Epoch 4/5\n",
      "2250000/2250000 [==============================] - 11944s 5ms/sample - loss: 0.2492 - acc: 0.8944 - val_loss: 0.3655 - val_acc: 0.8441\n",
      "Epoch 5/5\n",
      "2250000/2250000 [==============================] - 13344s 6ms/sample - loss: 0.2415 - acc: 0.8981 - val_loss: 0.3448 - val_acc: 0.8466\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from lstm import *\n",
    "model= LSTM_Model(all_tokens, use_gru=False, tensorboard=True)\n",
    "model.train_model(all_tokens, labels, wv, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"blstm_preGlove200_5epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.82      0.84    125553\n",
      "         1.0       0.83      0.86      0.84    124447\n",
      "\n",
      "    accuracy                           0.84    250000\n",
      "   macro avg       0.84      0.84      0.84    250000\n",
      "weighted avg       0.84      0.84      0.84    250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions<0] = 0\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_prob = None\n",
    "f1_max = 0\n",
    "\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    f1 = metrics.f1_score(y_test, (predictions > thresh).astype(int))\n",
    "    print('F1 score at threshold {} is {}'.format(thresh, f1), end='\\r')\n",
    "    \n",
    "    if f1 > f1_max:\n",
    "        f1_max = f1\n",
    "        opt_prob = thresh\n",
    "        \n",
    "print('Optimal probabilty threshold is {} for maximum F1 score {}'.format(opt_prob, f1_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep-wise Sep Convolutionnal NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_tokens = [list(filter(lambda i: i in wv, tweet)) for tweet in all_tokens]\n",
    "# Force the negative sentiment to be clasified to 0 instead of 1 \n",
    "labels = full_labels\n",
    "labels[labels<0] = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tokens, labels, test_size=0.1, train_size=0.1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 128, 200)          10947600  \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 128, 200)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_130 (Separa (None, 128, 64)           14264     \n",
      "_________________________________________________________________\n",
      "separable_conv1d_131 (Separa (None, 128, 64)           4608      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 18, 64)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_132 (Separa (None, 18, 128)           8768      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_133 (Separa (None, 18, 128)           17408     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_28  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 10,992,777\n",
      "Trainable params: 10,992,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2250000 samples, validate on 250000 samples\n",
      "Epoch 1/5\n",
      "2250000/2250000 [==============================] - 4890s 2ms/sample - loss: 0.3370 - acc: 0.8458 - val_loss: 0.3929 - val_acc: 0.8135\n",
      "Epoch 2/5\n",
      "2250000/2250000 [==============================] - 3762s 2ms/sample - loss: 0.2810 - acc: 0.8781 - val_loss: 0.3783 - val_acc: 0.8269\n",
      "Epoch 3/5\n",
      "2250000/2250000 [==============================] - 3666s 2ms/sample - loss: 0.2588 - acc: 0.8893 - val_loss: 0.2836 - val_acc: 0.8703\n",
      "Epoch 4/5\n",
      "2250000/2250000 [==============================] - 3550s 2ms/sample - loss: 0.2410 - acc: 0.8982 - val_loss: 0.3949 - val_acc: 0.8173\n",
      "Epoch 5/5\n",
      "2250000/2250000 [==============================] - 3756s 2ms/sample - loss: 0.2255 - acc: 0.9057 - val_loss: 0.4300 - val_acc: 0.8154\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Activation, GRU, LSTM, Bidirectional, Flatten, GlobalMaxPool1D\n",
    "\n",
    "activation = 'sigmoid'\n",
    "units = 1\n",
    "embedding_vectors= wv\n",
    "max_length = max([len(tweet_tokens) for tweet_tokens in all_tokens])\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(all_tokens)\n",
    "\n",
    "dropout_rate=0.2\n",
    "filters = 64\n",
    "kernel_size= 7\n",
    "pool_size=7\n",
    "learning_rate=1e-3\n",
    "\n",
    "\n",
    "# Transform each unique word in unique int identifier\n",
    "sequences = tokenizer_obj.texts_to_sequences(all_tokens)\n",
    "\n",
    "\n",
    "# Pad the tweet to have all the same size\n",
    "tweet_padded = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "\n",
    "# Construct our model with keras\n",
    "model = Sequential()\n",
    "\n",
    "# Add the embedding layer with our trained embedding matrix\n",
    "embedding_layer = Embedding(input_dim=embedding_vectors.syn0.shape[0] , output_dim=embedding_vectors.syn0.shape[1], weights=[embedding_vectors.syn0], \n",
    "                        input_length=tweet_padded.shape[1])\n",
    "\n",
    "model.add(embedding_layer)\n",
    "\n",
    "\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "model.add(SeparableConv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              padding='same'))\n",
    "model.add(SeparableConv1D(filters=filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          depthwise_initializer='random_uniform',\n",
    "                          kernel_initializer='glorot_uniform',\n",
    "                          padding='same'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "\n",
    "model.add(SeparableConv1D(filters=filters * 2,\n",
    "                      kernel_size=kernel_size,\n",
    "                      activation='relu',\n",
    "                      bias_initializer='random_uniform',\n",
    "                      depthwise_initializer='random_uniform',\n",
    "                      padding='same'))\n",
    "model.add(SeparableConv1D(filters=filters * 2,\n",
    "                      kernel_size=kernel_size,\n",
    "                      activation='relu',\n",
    "                      bias_initializer='random_uniform',\n",
    "                      depthwise_initializer='random_uniform',\n",
    "                      padding='same'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "print (model.summary())\n",
    "model.fit(tweet_padded, labels, batch_size=128, epochs=5, validation_split=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "model.save(\"sepCnn_preGlove200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250000 samples, validate on 250000 samples\n",
      "Epoch 1/2\n",
      "2250000/2250000 [==============================] - 9058s 4ms/sample - loss: 0.2126 - acc: 0.9117 - val_loss: 0.4133 - val_acc: 0.8355\n",
      "Epoch 2/2\n",
      " 304640/2250000 [===>..........................] - ETA: 53:52 - loss: 0.1930 - acc: 0.9203"
     ]
    }
   ],
   "source": [
    "# Transform each unique word in unique int identifier\n",
    "sequences = tokenizer_obj.texts_to_sequences(all_tokens)\n",
    "\n",
    "\n",
    "# Pad the tweet to have all the same size\n",
    "tweet_padded = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "model_1 = tf.keras.models.load_model(\"sepCnn_preGlove200\")\n",
    "model_1.fit(tweet_padded, labels, batch_size=128, epochs=2, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each unique word in unique int identifier\n",
    "sequences = tokenizer_obj.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the tweet to have all the same size\n",
    "tweet_padded = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "predictions = model.predict(tweet_padded)\n",
    "\n",
    "#print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.85      0.86    249914\n",
      "         1.0       0.85      0.87      0.86    250086\n",
      "\n",
      "    accuracy                           0.86    500000\n",
      "   macro avg       0.86      0.86      0.86    500000\n",
      "weighted avg       0.86      0.86      0.86    500000\n",
      "\n",
      "0.858672\n"
     ]
    }
   ],
   "source": [
    "pred = np.array(predictions, copy=True)\n",
    "threshold = 0.5\n",
    "\n",
    "pred[pred<threshold] = 0\n",
    "pred[pred>=threshold] = 1\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model on the entire dataset\n",
    "#clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "#clf.fit(all_tweets_vectors, full_labels)\n",
    "model_1 = tf.keras.models.load_model(\"sepCnn_preGlove200_7_0.865\")\n",
    "# Load the data to predict\n",
    "test_ids, test_x = load_csv_test_data(DATA_TEST_PATH, has_ID=True)\n",
    "\n",
    "# Tokenize it\n",
    "test_tokens = [tokenize(tweet) for tweet in test_x]\n",
    "#test_tokens = [list(filter(lambda i: i in wv, tweet)) for tweet in test_tokens]\n",
    "\n",
    "# Generate vector representation\n",
    "#all_tweets_vectors = np.array([generateTweetVector(model.wv, words) for words in test_tokens])\n",
    "# Transform each unique word in unique int identifier\n",
    "\n",
    "#sequences = tokenizer_obj.texts_to_sequences(test_tokens)\n",
    "\n",
    "# Pad the tweet to have all the same size\n",
    "#tweet_padded = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "predictions = model_1.predict(test_tokens)\n",
    "\n",
    "# Predict\n",
    "#predictions = model.predict(test_tokens)\n",
    "\n",
    "# Save predictions\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, predict_labels(predictions, 0.50), OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
