{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import *\n",
    "from tools import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import *\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "# Data input and output paths\n",
    "POS_TRAIN_PATH = '../data/twitter-datasets/train_pos_full.txt' \n",
    "NEG_TRAIN_PATH = '../data/twitter-datasets/train_neg_full.txt' \n",
    "DATA_TEST_PATH = '../data/twitter-datasets/test_data.txt'\n",
    "OUTPUT_PATH = 'predictions_out.csv'\n",
    "TOKENS_PATH = \"../saved_gen_files/all_tokens.txt\"\n",
    "W2V_MODEL_PATH = \"../saved_gen_files/w2v.model\"\n",
    "FastText_MODEL_PATH = \"../saved_gen_files/fasttext.model\"\n",
    "\n",
    "FULL_TRAIN_TWEET_VECTORS = \"../saved_gen_files/train_tweet_vectors.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids, pos_text_train = load_csv_test_data(POS_TRAIN_PATH)\n",
    "neg_ids, neg_text_train = load_csv_test_data(NEG_TRAIN_PATH)\n",
    "full_dataset = np.concatenate((pos_text_train, neg_text_train), axis=None)\n",
    "full_labels = np.concatenate((np.ones(len(pos_text_train)), -np.ones(len(pos_text_train))), axis=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top pos smiley used and not removed\n",
    "pos_smiley = [\"\\(':\", \"\\(':<\", \"\\(';\", \"\\(\\*:\", \"\\(\\*;\", \"\\(:\", \"\\(;\", \"\\(=\", \":'\\)\", \":'\\]\", \":'\\}\", \n",
    "              \":\\)\", \":\\*\\)\", \":\\*:\", \":-\\]\", \":-\\}\", \":\\]\", \":\\}\", \";'\\)\", \";'\\]\", \";\\)\", \";\\*\\)\", \";-\\}\"\n",
    "             , \";\\]\", \";\\}\", \"=\\)\", \"\\(=\", \"<3\", \":p\", \":D\", \"xD\", \":)\"]\n",
    "\n",
    "\n",
    "# Top neg smiley used and not removed\n",
    "neg_smiley = [\"\\)':\", \"\\)':<\", \"\\)';\", \"\\)=\", \"\\)=<\", \"/':\", \"/';\", \"/-:\", \"/:\", \"/:<\", \"/;\", \n",
    "              \"/;<\", \"/=\", \":'/\", \":'@\", \":'\\[\", \":'\\\\\", \":'\\{\", \":'\\|\", \":\\(\", \":\\*\\(\", \":\\*\\{\", \n",
    "            \":\\*\\|\", \":-/\", \":-@\", \":-\\[\", \":-\\\\\", \":-\\|\", \":/\", \":@\", \":\\[\", \":\\\\\", \":\\{\", \":\\|\"\n",
    "             , \";'\\(\", \";'/\", \";'\\[\", \";\\*\\{\", \";-/\", \";-\\|\", \";/\", \";@\", \";\\[\", \";\\\\\", \";\\{\", \";\\|\"\n",
    "             ,\"=\\(\", \"</3\"]\n",
    "\n",
    "\n",
    "# Top word without sentiment meaning nor negative form possible\n",
    "stop_words= [\"i\", \"you\", \"it\", \"she\", \"he\", \"we\", \"they\", \"a\", \"in\", \"to\", \"the\", \"and\", \"my\", \"me\", \"of\", \"for\", \"that\", \"this\", \"on\", \"so\", \"be\", \"just\", \"your\", \"at\", \"its\", \"im\", \".\", \",\", \")\", \"'\", \"(\", \"or\", \"by\", \"am\", \"ve\", \"our\", \"\\\"\", \"<\", \">\", \"&\", \"\\\\\", \":\", \"-\", \";\", \"/\"]\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "\n",
    "def processTweet(tweet):\n",
    "    # Tokenize\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in pos_smiley:\n",
    "            word = '<pos_smiley>'\n",
    "        elif word in neg_smiley:\n",
    "            word = '<neg_smiley>'\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stop_words and not word.isnumeric()]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "\n",
    "all_tokens = [tknzr.tokenize(tweet) for tweet in full_dataset]\n",
    "\n",
    "# Save \n",
    "with open(TOKENS_PATH, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(all_tokens, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOKENS_PATH, \"rb\") as fp:   # Unpickling\n",
    "    all_tokens = pickle.load(fp)\n",
    "tks = np.array(all_tokens)\n",
    "flat_list = [item for sublist in tks for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "dict_words = get_top_n_words(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(TOKENS_PATH, \"rb\") as fp:   # Unpickling\n",
    "#    all_tokens = pickle.load(fp)\n",
    "\n",
    "# Train a word2vec model to generate embedding\n",
    "model = models.Word2Vec(\n",
    "        all_tokens,\n",
    "        size=200,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        iter=10)\n",
    "model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOKENS_PATH, \"rb\") as fp:   # Unpickling\n",
    "    all_tokens = pickle.load(fp)\n",
    "\n",
    "# Train a word2vec model to generate embedding\n",
    "model = models.FastText(\n",
    "        all_tokens,\n",
    "        size=50,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        iter=10)\n",
    "\n",
    "model.save(FastText_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTweetVector(word_dic, words):\n",
    "    num_words = len(words)\n",
    "    if num_words < 1:\n",
    "        num_words = 1\n",
    "        \n",
    "    vector = np.zeros(word_dic.vector_size)\n",
    "    for word in words:\n",
    "        if word in word_dic.vocab:\n",
    "            vector += word_dic[word]\n",
    "    vector /= num_words\n",
    "    return vector\n",
    "\n",
    "all_tweets_vectors = np.array([generateTweetVector(model.wv, words) for words in all_tokens])\n",
    "\n",
    "# Save \n",
    "with open(FULL_TRAIN_TWEET_VECTORS, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(all_tweets_vectors, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FULL_TRAIN_TWEET_VECTORS, \"rb\") as fp:   # Unpickling\n",
    "    all_tweets_vectors = pickle.load(fp)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tweets_vectors, full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.80      0.75      0.77    312802\n",
      "         1.0       0.76      0.81      0.79    312198\n",
      "\n",
      "    accuracy                           0.78    625000\n",
      "   macro avg       0.78      0.78      0.78    625000\n",
      "weighted avg       0.78      0.78      0.78    625000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.Ridge(alpha=0.1)\n",
    "#clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(y_test, predict_labels(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v =  Word2Vec.load(W2V_MODEL_PATH)\n",
    "with open(TOKENS_PATH, \"rb\") as fp:   # Unpickling\n",
    "    all_tokens = pickle.load(fp)\n",
    "\n",
    "full_labels[full_labels<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 592851 unique tokens\n",
      "592852\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, GRU\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "max_length = max([len(tweet_tokens) for tweet_tokens in all_tokens])\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(all_tokens)\n",
    "sequences = tokenizer_obj.texts_to_sequences(all_tokens)\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens'% len(word_index))\n",
    "\n",
    "tweet_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>num_words:\n",
    "        continue\n",
    "    if word in w2v.wv.vocab:\n",
    "        embedding_matrix[i] = w2v.wv[word]\n",
    "        \n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (2000000, 128)\n",
      "y_train:  (2000000,)\n",
      "X_test:  (500000, 128)\n",
      "y_test:  (500000,)\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(tweet_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "tweet_pad=tweet_pad[indices]\n",
    "sentiment = full_labels[indices]\n",
    "#num_validatiion_samples = int(VALIDATION_SPLIT * tweet_pad.shape[0])\n",
    "\n",
    "#X_train = tweet_pad[:-num_validatiion_samples]\n",
    "#y_train = sentiment[:-num_validatiion_samples]\n",
    "\n",
    "#X_test = tweet_pad[-num_validatiion_samples:]\n",
    "#y_test = sentiment[-num_validatiion_samples:]\n",
    "\n",
    "#print(\"X_train: \", X_train.shape)\n",
    "#print(\"y_train: \", y_train.shape)\n",
    "#print(\"X_test: \", X_test.shape)\n",
    "#print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 128, 200)          118570400 \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 118,747,169\n",
      "Trainable params: 176,769\n",
      "Non-trainable params: 118,570,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2250000 samples, validate on 250000 samples\n",
      "Epoch 1/5\n",
      "2250000/2250000 [==============================] - 7788s 3ms/step - loss: 0.3589 - accuracy: 0.8370 - val_loss: 0.3778 - val_accuracy: 0.8137\n",
      "Epoch 2/5\n",
      "2250000/2250000 [==============================] - 7694s 3ms/step - loss: 0.3360 - accuracy: 0.8505 - val_loss: 0.3668 - val_accuracy: 0.8246\n",
      "Epoch 3/5\n",
      "2250000/2250000 [==============================] - 7773s 3ms/step - loss: 0.3302 - accuracy: 0.8534 - val_loss: 0.3459 - val_accuracy: 0.8319\n",
      "Epoch 4/5\n",
      "2250000/2250000 [==============================] - 7809s 3ms/step - loss: 0.3274 - accuracy: 0.8551 - val_loss: 0.3808 - val_accuracy: 0.8143\n",
      "Epoch 5/5\n",
      "2250000/2250000 [==============================] - 7833s 3ms/step - loss: 0.3255 - accuracy: 0.8561 - val_loss: 0.3707 - val_accuracy: 0.8121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nmodel.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\\nmodel.add(Dense(1, activation = 'sigmoid'))\\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.fit(X_train, y_train, batch_size=128, epochs=5, validation_data=[X_test, y_test],verbose=2)\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.layers import Dense, Dropout, Activation, GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 200, embeddings_initializer=Constant(embedding_matrix), input_length=max_length, trainable= False))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "print (model.summary())\n",
    "model.fit(tweet_pad, full_labels, batch_size=128, epochs=5, validation_split=0.1, shuffle=True, callbacks=[reduce_lr])\n",
    "'''\n",
    "\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=5, validation_data=[X_test, y_test],verbose=2)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00         0\n",
      "         0.0       0.00      0.00      0.00   1250000\n",
      "         1.0       0.79      0.86      0.82   1250000\n",
      "\n",
      "    accuracy                           0.43   2500000\n",
      "   macro avg       0.26      0.29      0.27   2500000\n",
      "weighted avg       0.40      0.43      0.41   2500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(TOKENS_PATH, \"rb\") as fp:   # Unpickling\n",
    "    all_tokens = pickle.load(fp)\n",
    "test_sentences = all_tokens\n",
    "test_sequences = tokenizer_obj.texts_to_sequences(test_sentences)\n",
    "test_tweet_pad = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "predictions = model.predict(x=test_tweet_pad)\n",
    "print(classification_report(full_labels, predict_labels(predictions, 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model on the entire dataset\n",
    "#clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "#clf.fit(all_tweets_vectors, full_labels)\n",
    "\n",
    "# Load the data to predict\n",
    "test_ids, test_x = load_csv_test_data(DATA_TEST_PATH, has_ID=True)\n",
    "\n",
    "# Tokenize it\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "test_tokens = [tknzr.tokenize(tweet) for tweet in test_x]\n",
    "\n",
    "# Generate vector representation\n",
    "#all_tweets_vectors = np.array([generateTweetVector(model.wv, words) for words in test_tokens])\n",
    "test_sequences = tokenizer_obj.texts_to_sequences(test_tokens)\n",
    "test_tweet_pad = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(test_tweet_pad)\n",
    "\n",
    "# Save predictions\n",
    "create_csv_submission(test_ids, predict_labels(predictions, 0.5), OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "test_tokens = [tknzr.tokenize(tweet) for tweet in [\"Great!! it is raining today!!\", \"I'm so sad\", \"I'm so happy\",\"love\"]]\n",
    "\n",
    "test_sequences = tokenizer_obj.texts_to_sequences([\"Great!! it is raining today!!\", \"I'm so sad\", \"I'm so happy\",\"love\"])\n",
    "test_tweet_pad = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(tweet_pad)\n",
    "preds_2 = predict_labels(preds, 0.5)\n",
    "print(classification_report(full_labels, preds_2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
