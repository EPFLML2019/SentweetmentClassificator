{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import *\n",
    "from tools import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim import *\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Data input and output paths\n",
    "POS_TRAIN_PATH = '../data/twitter-datasets/train_pos_full.txt' \n",
    "NEG_TRAIN_PATH = '../data/twitter-datasets/train_neg_full.txt' \n",
    "DATA_TEST_PATH = '../data/twitter-datasets/test_data.txt'\n",
    "OUTPUT_PATH = 'predictions_out.csv'\n",
    "TOKENS_PATH = \"../saved_gen_files/all_tokens.txt\"\n",
    "W2V_MODEL_PATH = \"../saved_gen_files/w2v.model\"\n",
    "FastText_MODEL_PATH = \"../saved_gen_files/fasttext.model\"\n",
    "\n",
    "FULL_TRAIN_TWEET_VECTORS = \"../saved_gen_files/train_tweet_vectors.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids, pos_text_train = load_csv_test_data(POS_TRAIN_PATH)\n",
    "neg_ids, neg_text_train = load_csv_test_data(NEG_TRAIN_PATH)\n",
    "full_dataset = np.concatenate((pos_text_train, neg_text_train), axis=None)\n",
    "full_labels = np.concatenate((np.ones(len(pos_text_train)), -np.ones(len(pos_text_train))), axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "#all_tokens = [tknzr.tokenize(tweet) for tweet in full_dataset]\n",
    "\n",
    "import re # for regular expressions\n",
    "from nltk.stem.porter import *\n",
    "from nltk import stem\n",
    "import nltk\n",
    "\n",
    "def stemHelper(words):\n",
    "    res = []\n",
    "    for word in words:\n",
    "        #print(word + \" \" + stemmer.lemmatize(word))\n",
    "        res.append(stemmer.lemmatize(word))\n",
    "\n",
    "    return res\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt\n",
    "\n",
    "pos_train_df = pd.DataFrame({'Label' : np.concatenate([np.array([-1 for _ in range(len(neg_text_train))]), np.ones(len(pos_text_train))]),\n",
    "                            'Text' : np.concatenate([neg_text_train, pos_text_train])})\n",
    "pos_train_df['clean_text'] = np.vectorize(remove_pattern)(pos_train_df['Text'], \"<[\\w]*>\") \n",
    "\n",
    "pos_train_df['clean_text'] = pos_train_df['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "pos_train_df['clean_text'] = pos_train_df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "pos_train_df['clean_text'].replace('', np.nan, inplace=True)\n",
    "pos_train_df.dropna(subset=['clean_text'], inplace=True)\n",
    "\n",
    "tokenized_tweet = [tknzr.tokenize(tweet) for tweet in pos_train_df['clean_text']] #tokenizing\n",
    "stemmer = stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "all_tokens = tokenized_tweet.apply(stemHelper) # stemming\n",
    "\n",
    "\n",
    "\n",
    "# Save \n",
    "#with open(TOKENS_PATH, \"wb\") as fp:   #Pickling\n",
    "#    pickle.dump(all_tokens, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOKENS_PATH, \"rb\") as fp:   # Unpickling\n",
    "    all_tokens = pickle.load(fp)\n",
    "\n",
    "# Train a word2vec model to generate embedding\n",
    "model = models.Word2Vec(\n",
    "        all_tokens,\n",
    "        size=50,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        iter=10)\n",
    "model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.9% 104.7/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model = api.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOKENS_PATH, \"rb\") as fp:   # Unpickling\n",
    "    all_tokens = pickle.load(fp)\n",
    "\n",
    "# Train a word2vec model to generate embedding\n",
    "model = models.FastText(\n",
    "        all_tokens,\n",
    "        size=50,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        iter=10)\n",
    "\n",
    "model.save(FastText_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTweetVector(word_dic, words):\n",
    "    num_words = len(words)\n",
    "    vector = np.zeros(word_dic.vector_size)\n",
    "    for word in words:\n",
    "        if word in word_dic.vocab:\n",
    "            vector += word_dic[word]\n",
    "    vector /= num_words\n",
    "    return vector\n",
    "\n",
    "all_tweets_vectors = np.array([generateTweetVector(model.wv, words) for words in all_tokens])\n",
    "\n",
    "# Save \n",
    "with open(FULL_TRAIN_TWEET_VECTORS, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(all_tweets_vectors, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_tweets_vectors, full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.74      0.69      0.71    312419\n",
      "         1.0       0.71      0.75      0.73    312581\n",
      "\n",
      "    accuracy                           0.72    625000\n",
      "   macro avg       0.72      0.72      0.72    625000\n",
      "weighted avg       0.72      0.72      0.72    625000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.Ridge(alpha=0.1)\n",
    "#clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(y_test, predict_labels(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model on the entire dataset\n",
    "clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clf.fit(all_tweets_vectors, full_labels)\n",
    "\n",
    "# Load the data to predict\n",
    "test_ids, test_x = load_csv_test_data(DATA_TEST_PATH, has_ID=True)\n",
    "\n",
    "# Tokenize it\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "test_tokens = [tknzr.tokenize(tweet) for tweet in test_x]\n",
    "\n",
    "# Generate vector representation\n",
    "all_tweets_vectors = np.array([generateTweetVector(model.wv, words) for words in test_tokens])\n",
    "\n",
    "# Predict\n",
    "predictions = clf.predict(all_tweets_vectors)\n",
    "\n",
    "# Save predictions\n",
    "create_csv_submission(test_ids, predictions, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
