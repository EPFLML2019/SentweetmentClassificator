{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import *\n",
    "from tools import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os.path\n",
    "\n",
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import *\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "\n",
    "# Data input and output paths\n",
    "POS_TRAIN_PATH = '../data/twitter-datasets/train_pos_full.txt' \n",
    "NEG_TRAIN_PATH = '../data/twitter-datasets/train_neg_full.txt' \n",
    "DATA_TEST_PATH = '../data/twitter-datasets/test_data.txt'\n",
    "OUTPUT_PATH = 'predictions_out.csv'\n",
    "\n",
    "TOKENS_PATH = \"../saved_gen_files/all_tokens.txt\"\n",
    "FULL_TRAIN_TWEET_VECTORS = \"../saved_gen_files/train_tweet_vectors.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids, pos_text_train = load_csv_test_data(POS_TRAIN_PATH)\n",
    "neg_ids, neg_text_train = load_csv_test_data(NEG_TRAIN_PATH)\n",
    "full_dataset = np.concatenate((pos_text_train, neg_text_train), axis=None)\n",
    "full_labels = np.concatenate((np.ones(len(pos_text_train)), -np.ones(len(pos_text_train))), axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(TOKENS_PATH):\n",
    "    with open(TOKENS_PATH, 'rb') as f:\n",
    "        all_tokens = pickle.load(f)\n",
    "else:    \n",
    "    all_tokens = [tokenizeTweet(tweet, stop_words=False, \n",
    "                           smiley_tag = False, strip_handles=True, \n",
    "                           reduce_len=True, preserve_case=False) for tweet in full_dataset]\n",
    "    \n",
    "    with open(TOKENS_PATH, 'wb') as f:\n",
    "        pickle.dump(all_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "Choose one of the embedding algo and the dimensions of the vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a word2vec model to generate embedding\n",
    "wv = getWord2VecDict(all_tokens, size=DIM, window=10, min_count=2, workers=10, iters=10, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load(\"glove-twitter-\" + DIM).wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a word2vec model to generate embedding\n",
    "wv = getFasttextDict(all_tokens, size=DIM, window=10, min_count=2, workers=10, iters=10, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweet in features with previous embedding system\n",
    "all_tweets_vectors = generateTweetsFeatures(all_tokens, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tweets_vectors, full_labels, test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and test the model\n",
    "clf = linear_model.Ridge(alpha=0.1)\n",
    "#clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(y_test, predict_labels(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the negative sentiment to be clasified to 0 instead of 1 \n",
    "labels = full_labels\n",
    "labels[labels<0] = 0\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tokens, labels, test_size=0.1, train_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 82, 50)            11779500  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 82, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,879,469\n",
      "Trainable params: 11,879,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/5\n",
      "450000/450000 [==============================] - 579s 1ms/step - loss: 0.5339 - accuracy: 0.7041 - val_loss: 0.4479 - val_accuracy: 0.7757\n",
      "Epoch 2/5\n",
      "450000/450000 [==============================] - 586s 1ms/step - loss: 0.4709 - accuracy: 0.7591 - val_loss: 0.4277 - val_accuracy: 0.7935\n",
      "Epoch 3/5\n",
      "450000/450000 [==============================] - 594s 1ms/step - loss: 0.4464 - accuracy: 0.7770 - val_loss: 0.4035 - val_accuracy: 0.8070\n",
      "Epoch 4/5\n",
      "450000/450000 [==============================] - 647s 1ms/step - loss: 0.4317 - accuracy: 0.7890 - val_loss: 0.3937 - val_accuracy: 0.8127\n",
      "Epoch 5/5\n",
      "450000/450000 [==============================] - 594s 1ms/step - loss: 0.4181 - accuracy: 0.7980 - val_loss: 0.3945 - val_accuracy: 0.8166\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from lstm import *\n",
    "model= LSTM_Model(X_train)\n",
    "model.train_model(X_train, y_train, wv, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.73      0.80    249783\n",
      "         1.0       0.77      0.90      0.83    250217\n",
      "\n",
      "    accuracy                           0.81    500000\n",
      "   macro avg       0.82      0.81      0.81    500000\n",
      "weighted avg       0.82      0.81      0.81    500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions<0] = 0\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model on the entire dataset\n",
    "#clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "#clf.fit(all_tweets_vectors, full_labels)\n",
    "\n",
    "# Load the data to predict\n",
    "test_ids, test_x = load_csv_test_data(DATA_TEST_PATH, has_ID=True)\n",
    "\n",
    "# Tokenize it\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "test_tokens = [tknzr.tokenize(tweet) for tweet in test_x]\n",
    "\n",
    "# Generate vector representation\n",
    "#all_tweets_vectors = np.array([generateTweetVector(model.wv, words) for words in test_tokens])\n",
    "test_sequences = tokenizer_obj.texts_to_sequences(test_tokens)\n",
    "test_tweet_pad = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(test_tweet_pad)\n",
    "\n",
    "# Save predictions\n",
    "create_csv_submission(test_ids, predict_labels(predictions, 0.5), OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
